---
title: "502 Project, Cross-Validation Setup"
author: "Madeline Chang"
output:
 pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r}
library(tidyverse)
library(dplyr)
library(forcats)
library(ggplot2)
library(rpart)
library(caret)
library(rpart.plot)
library(C50)
library(Metrics)
library(e1071)
library(glmnet)
library(nnet)
library(NeuralNetTools)
library(fastDummies)
library(class)
library(naivebayes)
library(pROC)
```

Reading in data

```{r}
recruitment<- read.csv('/Users/mtc/ADS/ADS 502/Project/recruitment_data.csv')

recruitment<- recruitment %>%
  mutate(Gender = as.factor(Gender),
         EducationLevel = as.factor(EducationLevel),
         RecruitmentStrategy = as.factor(RecruitmentStrategy),
         HiringDecision = as.factor(HiringDecision),
         Gender_name = as.factor(ifelse(Gender == 0, "Male", "Female")),
         Hiring_name = as.factor(ifelse(HiringDecision == 0, "Not_Hired", "Hired")),
         Recruitment_name = fct_collapse(RecruitmentStrategy,
                                        Aggressive = 1,
                                        Moderate = 2,
                                        Conservative = 3),
         Education_name = fct_collapse(EducationLevel,
                                       Bachelors = 1,
                                       Bachelor_2 = 2,
                                       Masters = 3,
                                       PhD = 4))
```

### Splitting Data

```{r}
set.seed(720)

trainingRows <- createDataPartition(recruitment$Hiring_name, p = .75, list = FALSE) 

recruit_train <- recruitment[trainingRows, c(1, 4, 5, 6, 7, 8, 9, 12, 14, 15, 13)]
recruit_test <- recruitment[-trainingRows, c(1, 4, 5, 6, 7, 8, 9, 12, 14, 15, 13)]
```

### Creating Dummy Variables for Categorical Attributes

```{r}
non_num_train<- recruit_train %>%
  select(Gender_name, Education_name, Recruitment_name) %>%
  dummy_cols() %>%
  select(-c(1:3))

colnames(non_num_train)<- c("Female", "Male", "Bachelors1", "Bachelors2", "Masters", "PhD", "Agressive", "Moderate", "Conservative")

non_num_test<- recruit_test %>%
  select(Gender_name, Education_name, Recruitment_name) %>%
  dummy_cols() %>%
  select(-c(1:3))

colnames(non_num_test)<- c("Female", "Male", "Bachelors1", "Bachelors2", "Masters", "PhD", "Agressive", "Moderate", "Conservative")
```

### Combining Numeric and Dummy Variables into Train/Test Sets

```{r}
recruit_train_n<- non_num_train %>%
  cbind(recruit_train[,c(1, 2, 3, 4, 5, 6, 7)], recruit_train$Hiring_name) %>%
  rename(HiringDecision = "recruit_train$Hiring_name") 

recruit_test_n<- non_num_test %>%
  cbind(recruit_test[,c(1, 2, 3, 4, 5, 6, 7)], recruit_test$Hiring_name) %>%
  rename(HiringDecision = "recruit_test$Hiring_name") 
```

### Model Building Function

```{r}
# Function for classification methods without tuning parameters
class_function<- function(method){
  model<- train(x = recruit_train_n[,1:16], 
               y = recruit_train_n$HiringDecision,
               method = method,
               preProc = c("center", "scale"),
               metric = "ROC",
               trControl = ctrl)
}

# Function for classification methods with a tuning grid
class_grid<- function(method, grid, ctrl){
  model<- train(x = recruit_train_n[,1:16], 
               y = recruit_train_n$HiringDecision,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               metric = "ROC",
               trControl = ctrl)
}

# Function for KNN classification
class_knn<- function(method, length, ctrl){
  model<- train(x = recruit_train_n[,1:16], 
               y = recruit_train_n$HiringDecision,
               method = method,
               preProc = c("center", "scale"),
               tuneLength = length,
               metric = "ROC",
               trControl = ctrl)
}

# Function for Random Forest classification
class_rf<- function(method, grid, ntree, ctrl){
  model<- train(x = recruit_train_n[,1:16], 
               y = recruit_train_n$HiringDecision,
               method = method,
               preProc = c("center", "scale"),
               tuneGrid = grid,
               ntree = ntree,
               metric = "ROC",
               trControl = ctrl)
}
```

### Setting Controls and Hyperparameter Tuning

```{r}
ctrl <- trainControl(method = "cv",
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE,
                     savePredictions = TRUE)

nnet_grid <- expand.grid(size=1:4, decay=c(2, 2.5, 3, 3.5, 4, 4.5, 5))

mtryValues <- data.frame(mtry = seq(5,15,1))
```


## Modeling

### Logistic Regression

```{r}
set.seed(720)
glm_train<- class_function("glm")
```

### Naive Bayes

```{r}
set.seed(720)
nb_train<- class_function("naive_bayes")
```

### KNN Model

```{r}
set.seed(720)
knn_train<- class_knn("knn", 20, ctrl)

knn_train
```

### Neural Network Model

```{r}
set.seed(720)
nnet_train<- class_grid("nnet", nnet_grid, ctrl)

nnet_train
```

## Model Performance on the Training Dataset Using ROC

```{r}
model_roc <- function(model){
  roc(response = model$pred$obs,
             predictor = model$pred$Hired,
             levels = rev(levels(model$pred$obs)))
}

glm_roc<- model_roc(glm_train)
nb_roc<- model_roc(nb_train)
knn_roc<- model_roc(knn_train)
nnet_roc<- model_roc(nnet_train)

plot(glm_roc, type = "s", col = 'red', legacy.axes = TRUE, 
     main = "Hiring Decision ROC Curve")
plot(nb_roc, type = "s", add = TRUE, col = 'blue', legacy.axes = TRUE)
plot(knn_roc, type = "s", add = TRUE, col = 'pink', legacy.axes = TRUE)
plot(nnet_roc, type = "s", add = TRUE, col = 'yellow', legacy.axes = TRUE)
legend("bottomright", legend=c(paste("LR (AUC =", round(glm_roc$auc,4), ")"), 
                               paste("NB (AUC =", round(nb_roc$auc,4), ")"), 
                               paste("KNN (AUC =", round(knn_roc$auc,4), ")"), 
                               paste("NNET (AUC =", round(nnet_roc$auc,4), ")")),
       col=c("red", "blue", "pink", "yellow"), lwd=2)
```


```{r}
train_re<- resamples(list(
  glm = glm_train,
  nb = nb_train,
  knn = knn_train,
  nnet = nnet_train
))

dotplot(train_re, metric = "ROC")
```

## Applying Models to Test Dataset

```{r}
testResults <- data.frame(
  obs = recruit_test_n$HiringDecision,
  glm = predict(glm_train, recruit_test_n[, 1:16]),
  nb = predict(nb_train, recruit_test_n[, 1:16]),
  knn = predict(knn_train, recruit_test_n[, 1:16]),
  nnet = predict(nnet_train, recruit_test_n[, 1:16])
)
```

```{r}
confusionMatrix(testResults$obs, testResults$glm)

confusionMatrix(testResults$obs, testResults$nb)

confusionMatrix(testResults$obs, testResults$knn)

confusionMatrix(testResults$obs, testResults$nnet)
```



